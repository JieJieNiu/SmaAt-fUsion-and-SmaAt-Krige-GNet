{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e753bf-01bf-4d41-af69-7aba1d39b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "from torch import nn, triangular_solve\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from root import ROOT_DIR\n",
    "from utils import data_loader_precip, dataset_precip, data_loader_precip, dataset_hybrid\n",
    "from utils.model_classes import get_model_class\n",
    "from models import unet_precip_regression_lightning as unet_regr\n",
    "import glob\n",
    "NORM = 47.83\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca210f2d-1b63-4420-b255-31a6055785f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.split_sizes = [\n",
    "        12 * 22 * 8,  # Size of tensor1 after flattening\n",
    "        12 * 64 * 64,  # Size of tensor2 after flattening\n",
    "        64 * 64,              # Size of tensor3 after flattening\n",
    "    ]\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.tensor(input).to('cuda')\n",
    "        print(input.shape)\n",
    "        b = input.shape[0]\n",
    "        flat_nodes, flat_images, flat_target = torch.split(input, self.split_sizes, dim=1)\n",
    "        print(flat_nodes.size)\n",
    "        print(flat_images.size)\n",
    "        print(flat_target.size)\n",
    "\n",
    "        input_nodes = flat_nodes.reshape((b, 12, 22, 8))\n",
    "        input_images = flat_images.reshape((b, 12, 64, 64))\n",
    "        y_true = flat_target.reshape((b, 64, 64))\n",
    "        # Forward pass through the model to get predictions\n",
    "        y_pred = self.model(input_images, input_nodes)\n",
    "\n",
    "        # Compute the loss: MSE loss with per-sample losses\n",
    "        loss = nn.functional.mse_loss(y_pred.squeeze(), y_true.squeeze(), reduction=\"none\")\n",
    "        mean_loss = loss.mean(dim=1)\n",
    "        return mean_loss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27e708b-a60c-4c03-843e-90887f5a096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 12, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_hybrid.precipitation_maps_h5_nodes(\n",
    "    in_file= ROOT_DIR / \"data\" / \"precipitation\" / \"Node_2016-2019.h5\",\n",
    "    num_input_images=12,\n",
    "    num_output_images=6, \n",
    "    train=False,\n",
    "    return_timestamp = False)\n",
    "\n",
    "indices = np.random.choice(len(dataset), 100, replace=False)\n",
    "subset = Subset(dataset, indices)\n",
    "\n",
    "input_images = []\n",
    "input_nodes = []\n",
    "target_images = []\n",
    "\n",
    "for i in subset:\n",
    "    input_img, input_2, target_img, target_2 = i\n",
    "    input_images.append(torch.tensor(input_img, dtype=torch.float32).to('cuda'))\n",
    "    input_nodes.append(torch.tensor(input_2).to('cuda'))\n",
    "    target_images.append(torch.tensor(target_img, dtype=torch.float32).to('cuda'))\n",
    "\n",
    "input_images = torch.stack(input_images)\n",
    "input_nodes = torch.stack(input_nodes)\n",
    "target_images = torch.stack(target_images)\n",
    "print(input_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c976a4-8277-4116-b2f2-186085ceb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 55360])\n",
      "<built-in method size of Tensor object at 0x7f473232ff60>\n",
      "<built-in method size of Tensor object at 0x7f473232fc40>\n",
      "<built-in method size of Tensor object at 0x7f473232fce0>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Bridge\"\n",
    "model_folder = ROOT_DIR / \"comparison\" / model_name\n",
    "model, model_name = get_model_class(model_name)\n",
    "models = [m for m in os.listdir(model_folder) if \"small\" in m]\n",
    "model_file = models[0]\n",
    "model = model.load_from_checkpoint(f\"{model_folder}/{model_file}\")\n",
    "\n",
    "model = ModelWrapper(model)\n",
    "model.eval()\n",
    "#FLATTEN INPUT\n",
    "flat_nodes = input_nodes.view(input_nodes.size(0), -1)  # Flatten from the 2nd dimension onward\n",
    "flat_images = input_images.view(input_images.size(0), -1)\n",
    "flat_target = target_images.view(target_images.size(0), -1)\n",
    "\n",
    "flattened = torch.cat((flat_nodes, flat_images, flat_target), axis=1)[:5]\n",
    "\n",
    "# Create the masker for input_nodes\n",
    "explainer = shap.KernelExplainer(model, flattened[:2].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5afdd6-01ab-49e1-bf5b-564e7eebba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 12, 22, 8])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31edc96536e4a0fa18f3c166b692a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 55360])\n",
      "<built-in method size of Tensor object at 0x7f47322a6c50>\n",
      "<built-in method size of Tensor object at 0x7f47336bbb50>\n",
      "<built-in method size of Tensor object at 0x7f4733983b50>\n"
     ]
    }
   ],
   "source": [
    "print(input_nodes.shape)\n",
    "shap_values = explainer.shap_values(flattened.cpu().numpy()[3:4])\n",
    "print(\"SHAP values shape:\", shap_values.shape)\n",
    "node_shap_values = shap_values[:,:2112]\n",
    "shap.summary_plot(node_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f2ae2-7e53-456d-8a25-e1a23cb79540",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_nodes.shape)\n",
    "print(input_images.shape)\n",
    "print(target_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b21c53-717e-4ebb-987b-2edb3ee0db7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4680e6-e472-4aac-9b03-ed2ac33d1610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
